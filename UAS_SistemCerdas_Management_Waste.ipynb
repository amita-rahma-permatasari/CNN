{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1UbGWYFm36uSikVZMvFPOzacP7YYwJqvi",
      "authorship_tag": "ABX9TyOfRWMfUrhmAG2yWD+I+o/A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amita-rahma-permatasari/CNN/blob/main/UAS_SistemCerdas_Management_Waste.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Import Libraries***"
      ],
      "metadata": {
        "id": "ZxdBrT5ODUQr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oNF3BniGCgw_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import urllib\n",
        "import itertools\n",
        "import random, os, glob\n",
        "import pickle\n",
        "from imutils import paths\n",
        "from sklearn.utils import shuffle\n",
        "from urllib.request import urlopen\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, MaxPooling2D, Dense, Dropout, SpatialDropout2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img, array_to_img\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Mounting Google Drive***"
      ],
      "metadata": {
        "id": "dk3JLnk5Eg7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ft8CgvPvEi6v",
        "outputId": "7b887ad0-c9d2-4607-e2dd-b56d284d05eb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Import Dataset***"
      ],
      "metadata": {
        "id": "y0Gkh-oeFDko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"/content/drive/MyDrive/Garbage classification/Garbage classification\"\n",
        "target_size = (224, 224)"
      ],
      "metadata": {
        "id": "FtVeSJ4iFV2l"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "waste_labels = {\"cardboard\": 0, \"glass\": 1, \"metal\": 2, \"paper\": 3, \"plastic\": 4, \"trash\": 5}"
      ],
      "metadata": {
        "id": "W9kE6-qkFiub"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(base_dir):\n",
        "    print(f\"The directory {base_dir} exists.\")\n",
        "\n",
        "    # List the contents of the directory\n",
        "    print(\"Contents of the directory:\")\n",
        "    for item in os.listdir(base_dir):\n",
        "        print(item)\n",
        "else:\n",
        "    print(f\"The directory {base_dir} does not exist.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEPXku3cFq-0",
        "outputId": "12b4c7cf-deb3-4bcc-d35e-7199936df446"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The directory /content/drive/MyDrive/Garbage classification/Garbage classification exists.\n",
            "Contents of the directory:\n",
            "cardboard\n",
            "glass\n",
            "paper\n",
            "plastic\n",
            "trash\n",
            "metal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Load Dataset***"
      ],
      "metadata": {
        "id": "vsb8FpB8F3CQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(path):\n",
        "    x = []\n",
        "    labels = []\n",
        "    image_paths = sorted(list(paths.list_images(path)))\n",
        "    for image_path in image_paths:\n",
        "        img = cv2.imread(image_path)\n",
        "        img = cv2.resize(img, target_size)\n",
        "        x.append(img)\n",
        "        label = image_path.split(os.path.sep)[-2]\n",
        "        labels.append(waste_labels[label])\n",
        "\n",
        "    x, labels = shuffle(x, labels, random_state=42)\n",
        "    input_shape = (np.array(x[0]).shape[0], np.array(x[0]).shape[1], 3)\n",
        "\n",
        "    print(\"X shape: \", np.array(x).shape)\n",
        "    print(f\"Number of Labels: {len(np.unique(labels))} , Number of Observations: {len(labels)}\")\n",
        "    print(\"Input Shape: \", input_shape)\n",
        "\n",
        "    return np.array(x), np.array(labels), input_shape"
      ],
      "metadata": {
        "id": "EgUjFISyF-B_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, labels, input_shape = load_dataset(base_dir)"
      ],
      "metadata": {
        "id": "Ng5PJ8v3GJD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = x / 255.0"
      ],
      "metadata": {
        "id": "7uLwTmC7Gcu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Data Preparation***"
      ],
      "metadata": {
        "id": "aIiEdieIHepu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CNN_data_preparation():\n",
        "  train = ImageDataGenerator(horizontal_flip=True,\n",
        "                             vertical_flip=True,\n",
        "                             validation_split=0.1,\n",
        "                             rescale=1./255,\n",
        "                             shear_range=0.1,\n",
        "                             zoom_range=0.1,\n",
        "                             width_shift_range=0.1,\n",
        "                             height_shift_range=0.1)\n",
        "  test = ImageDataGenerator(rescale=1/255, validation_split=0.1)\n",
        "  train_generator = train.flow_from_directory(directory=base_dir,\n",
        "                                              target_size=(target_size),\n",
        "                                              class_mode=\"categorical\",\n",
        "                                              subset=\"training\")\n",
        "  test_generator = test.flow_from_directory(directory=base_dir,\n",
        "                                            target_size=(target_size),\n",
        "                                            batch_size=251,\n",
        "                                            class_mode=\"categorical\",\n",
        "                                            subset=\"validation\")\n",
        "  return train_generator, test_generator"
      ],
      "metadata": {
        "id": "dwHitEKaHqS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator, test_generator = CNN_data_preparation()"
      ],
      "metadata": {
        "id": "MZEdG2kpJ5kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "total = 0\n",
        "# Access the directory path from the DirectoryIterator object\n",
        "for category in os.listdir(train_generator.directory):\n",
        "    count= 0\n",
        "    for image in os.listdir(os.path.join(train_generator.directory, category)):\n",
        "        count += 1\n",
        "        total +=1\n",
        "    print(str(category).title() + \": \" + str(count))\n",
        "print(f\"\\nTotal number of train images: {total}\")"
      ],
      "metadata": {
        "id": "_iHdC7icKrb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'train_generator' is defined and accessible from previous cells\n",
        "class_names = list(train_generator.class_indices.keys())  # Get class names\n",
        "class_dis = []  # Placeholder for class distribution\n",
        "\n",
        "# Calculate class distribution\n",
        "for category in class_names:\n",
        "    category_path = os.path.join(train_generator.directory, category)\n",
        "    count = len(os.listdir(category_path))\n",
        "    class_dis.append(count)\n",
        "\n",
        "# Create DataFrame\n",
        "DF = pd.DataFrame({\n",
        "    'Class names': class_names,\n",
        "    'Count': class_dis\n",
        "})\n",
        "\n",
        "# Plot with color palette\n",
        "plt.figure(figsize=(12, 9))\n",
        "palette = sns.color_palette(\"viridis\", len(class_names))  # You can change \"viridis\" to any other palette\n",
        "ax = sns.barplot(x='Class names', y='Count', data=DF, palette=palette)\n",
        "ax.bar_label(ax.containers[0])\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
        "\n",
        "# Add title and labels with explicit font sizes\n",
        "plt.title('Class Distribution', fontsize=16)\n",
        "plt.xlabel('Class Names', fontsize=14)\n",
        "plt.ylabel('Count', fontsize=14)\n",
        "\n",
        "# Add annotations for each bar\n",
        "for container in ax.containers:\n",
        "    ax.bar_label(container, fontsize=12, color='black')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5Y0UsohwMygq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Visualization Image***"
      ],
      "metadata": {
        "id": "7z8XeIOSGO2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualization_img(image_batch, labels, num_of_img):\n",
        "  plt.figure(figsize=(10,10))\n",
        "  for n in range(num_of_img):\n",
        "    ax = plt.subplot(5,5,n+1)\n",
        "    plt.imshow(image_batch[n])\n",
        "    plt.title(np.array(list(waste_labels.keys()))[to_categorical(labels, num_classes=len(np.unique(labels)))[n] == 1][0].title())\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "evv1fAmXHEn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualization_img(x, labels, 10)"
      ],
      "metadata": {
        "id": "79jCMZJ9HQnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Modelling***"
      ],
      "metadata": {
        "id": "OgyDW1xoPCmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CNN_create_and_fit_model(train_generator, test_generator, summary=True, fit=True, epochs=10):\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Conv2D(filters=32, kernel_size=(3,3), padding=\"same\", input_shape=(input_shape), activation=\"relu\"))\n",
        "  model.add(MaxPooling2D(pool_size=2, strides=(2,2)))\n",
        "\n",
        "  model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "  model.add(MaxPooling2D(pool_size=2, strides=(2,2)))\n",
        "\n",
        "  model.add(Conv2D(filters=32, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "  model.add(MaxPooling2D(pool_size=2, strides=(2,2)))\n",
        "\n",
        "  model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "  model.add(MaxPooling2D(pool_size=2, strides=(2,2)))\n",
        "\n",
        "  model.add(Conv2D(filters=32, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "  model.add(MaxPooling2D(pool_size=2, strides=(2,2)))\n",
        "\n",
        "  model.add(Flatten())\n",
        "\n",
        "  model.add(Dense(units=64, activation=\"relu\"))\n",
        "  model.add(Dropout(rate=0.2))\n",
        "\n",
        "  model.add(Dense(units=32, activation=\"relu\"))\n",
        "  model.add(Dropout(rate=0.2))\n",
        "\n",
        "  model.add(Dense(units=64, activation=\"relu\"))\n",
        "  model.add(Dropout(rate=0.2))\n",
        "\n",
        "  model.add(Dense(units=32, activation=\"relu\"))\n",
        "  model.add(Dropout(rate=0.2))\n",
        "\n",
        "  model.add(Dense(units=6, activation=\"softmax\"))\n",
        "\n",
        "  model.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer=\"adam\",\n",
        "                metrics=[tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), \"acc\"])\n",
        "\n",
        "  callbacks = [EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1, mode=\"min\"),\n",
        "              ModelCheckpoint(filepath=\"mymodel.h5\", monitor=\"val_loss\", mode=\"min\", save_best_only=True, save_weights_only=False, verbose=1)]\n",
        "\n",
        "  if summary:\n",
        "    model.summary()\n",
        "\n",
        "  if fit:\n",
        "    history = model.fit_generator(generator=train_generator, epochs=epochs, validation_data=test_generator,\n",
        "                                callbacks=callbacks, workers=4, steps_per_epoch=700, validation_steps=251//32) # steps_per_epoch=2276//32, validation_steps=251//32\n",
        "                                                                                                              # you can choose 2276/32 is 71 per epoch for best accuracy\n",
        "\n",
        "  return model, history"
      ],
      "metadata": {
        "id": "eSJgmnnaPMFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, history = CNN_create_and_fit_model(train_generator, test_generator)"
      ],
      "metadata": {
        "id": "LMuZv9YwPciH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Evaluation***"
      ],
      "metadata": {
        "id": "YlqC3Ye6PuRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def CNN_model_evaluate(model):\n",
        "  loss, precision, recall, acc = model.evaluate(test_generator, batch_size=32)\n",
        "  print(\"Test Accuracy: %.2f\" % (100 * acc))\n",
        "  print(\"Test Loss: %.2f\" % (100 * loss))\n",
        "  print(\"Test Precision: %.2f\" % (100 * precision))\n",
        "  print(\"Test Recall: %.2f\" % (100 * recall))\n",
        "\n",
        "  X_test, y_test = test_generator.next()\n",
        "  y_pred = model.predict(X_test)\n",
        "  y_pred = np.argmax(y_pred, axis=1)\n",
        "  y_test = np.argmax(y_test, axis=1)\n",
        "  target_names = list(waste_labels.keys())\n",
        "  print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "\n",
        "  plt.figure(figsize=(20,5))\n",
        "  plt.subplot(1,2,1)\n",
        "  plt.plot(history.history[\"acc\"], color=\"r\", label=\"Training Accuracy\")\n",
        "  plt.plot(history.history[\"val_acc\"], color=\"b\", label=\"Validation Accuracy\")\n",
        "  plt.legend(loc=\"lower right\")\n",
        "  plt.xlabel(\"Epoch\", fontsize=16)\n",
        "  plt.ylabel(\"Accuracy\", fontsize=16)\n",
        "  plt.ylim([min(plt.ylim()),1])\n",
        "  plt.title(\"Training and Validation Accuracy\", fontsize=16)\n",
        "\n",
        "  plt.subplot(1,2,2)\n",
        "  plt.plot(history.history[\"loss\"], color=\"r\", label=\"Training Loss\")\n",
        "  plt.plot(history.history[\"val_loss\"], color=\"b\", label=\"Validation Loss\")\n",
        "  plt.legend(loc=\"upper right\")\n",
        "  plt.xlabel(\"Epoch\", fontsize=16)\n",
        "  plt.ylabel(\"Loss\", fontsize=16)\n",
        "  plt.ylim([0, max(plt.ylim())])\n",
        "  plt.title(\"Training and Validation Loss\", fontsize=16)\n",
        "\n",
        "  return y_test, y_pred"
      ],
      "metadata": {
        "id": "QaLPHzRfNWOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test, y_pred = CNN_model_evaluate(model)"
      ],
      "metadata": {
        "id": "u_N0uPABNYA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Confusion Matrix***"
      ],
      "metadata": {
        "id": "CQAlvLhWT-bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(y_test, y_pred)\n",
        "def plot_confusion_matrix(cm, classes, normalize=False, title=\"Confusion Matrix\", cmap=plt.cm.OrRd):\n",
        "  if normalize:\n",
        "    cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "  plt.figure(figsize=(8,6))\n",
        "  plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
        "  plt.title(title)\n",
        "  plt.colorbar()\n",
        "  tick_marks = np.arange(len(classes))\n",
        "  plt.xticks(tick_marks, classes, rotation=45)\n",
        "  plt.yticks(tick_marks, classes)\n",
        "  fmt = \".2f\" if normalize else \"d\"\n",
        "  thresh = cm.max() / 2.\n",
        "  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "    plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\",\n",
        "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "  plt.tight_layout()\n",
        "  plt.ylabel(\"True Labels\", fontweight=\"bold\")\n",
        "  plt.xlabel(\"Predicted Labels\", fontweight=\"bold\")"
      ],
      "metadata": {
        "id": "vRyKjs-uUJQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(cm, waste_labels.keys())"
      ],
      "metadata": {
        "id": "pyt-bfkBUb6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Model Testing***"
      ],
      "metadata": {
        "id": "MPJCj05DU99K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import image\n",
        "def CNN_model_testing(path):\n",
        "  img = image.load_img(path, target_size=(target_size))\n",
        "  img = image.img_to_array(img, dtype=np.uint8)\n",
        "  img = np.array(img)/255.0\n",
        "  p = model.predict(img.reshape(1,224,224,3))\n",
        "  predicted_class = np.argmax(p[0])\n",
        "  return img, p, predicted_class"
      ],
      "metadata": {
        "id": "Hhj24JJmU_ta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img, p, predicted_class = CNN_model_testing(\"/content/drive/MyDrive/Garbage classification/test6.jpg\")\n"
      ],
      "metadata": {
        "id": "NEUmLSMRVDE3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "waste_labels = {0:\"cardboard\", 1:\"glass\", 2:\"metal\", 3:\"paper\", 4:\"plastic\", 5:\"trash\"}\n",
        "def plot_model_testing(img, p, predicted_class):\n",
        "  plt.axis(\"off\")\n",
        "  plt.imshow(img.squeeze())\n",
        "  plt.title(\"Maximum Probabilty: \" + str(np.max(p[0], axis=-1)) + \"\\n\" + \"Predicted Class: \" + str(waste_labels[predicted_class]))\n",
        "  plt.imshow(img);"
      ],
      "metadata": {
        "id": "pdCFjEtIVbYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model_testing(img, p, predicted_class)"
      ],
      "metadata": {
        "id": "bpTHm2eDVhpT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}